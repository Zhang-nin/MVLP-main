{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# !pip install ftfy regex tqdm\n",
    "# !pip install git+https://github.com/openai/CLIP.git\n",
    "# !pip install pandas\n",
    "# !pip install transformers\n",
    "# !pip install scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Import all the dependencies\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\n",
    "from pathlib import Path\n",
    "import clip\n",
    "import copy\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the LOC features \n",
    "all_ROI = torch.load(\"autodl-tmp/Dataset/feature/all_ocr_tensor.pt\")\n",
    "all_face = torch.load(\"autodl-tmp/Dataset/feature/all_face_tensor.pt\")\n",
    "\n",
    "# Load the ENT features\n",
    "all_ENT = torch.load(\"autodl-tmp/Dataset/feature/all_text_tensor.pt\")\n",
    "\n",
    "# Load the Caption features\n",
    "all_CAP = torch.load(\"autodl-tmp/Dataset/feature/all_cap_tensor.pt\")\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "data_dir = \"autodl-tmp/Dataset/images\"\n",
    "# train_path = \"autodl-tmp/Dataset/test/fewshot.json\"\n",
    "train_path = \"autodl-tmp/Dataset/test/train.json\"\n",
    "dev_path = \"autodl-tmp/Dataset/test/val.json\"\n",
    "test_path = \"autodl-tmp/Dataset/test/test.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1181</td>\n",
       "      <td>1181.jpg</td>\n",
       "      <td>[2, 1, 2]</td>\n",
       "      <td>I hate children \\n they are loud and messy and...</td>\n",
       "      <td>a brown and white cat with blue eyes being held</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>425</td>\n",
       "      <td>425.jpg</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>One simply does not run load bank cables \\n wi...</td>\n",
       "      <td>a man with long hair and a leather jacket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12742</td>\n",
       "      <td>12742.jpg</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>IF YOU DON'T FOLLOW THE \\n RULES \\n YOU'RE GON...</td>\n",
       "      <td>if you dont follow the rules youre gonna have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11472</td>\n",
       "      <td>11472.jpg</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>YEA LEMME GET THE \\n LAST SUPPER FADER \\n</td>\n",
       "      <td>a tattoo of the last supper and a black and wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2741</td>\n",
       "      <td>2741.jpg</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>Mrs. Reimus when someone gets \\n the right answer</td>\n",
       "      <td>a portrait of a man in a hat with his hand out</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id        img     labels  \\\n",
       "0   1181   1181.jpg  [2, 1, 2]   \n",
       "1    425    425.jpg  [1, 1, 1]   \n",
       "2  12742  12742.jpg  [0, 0, 0]   \n",
       "3  11472  11472.jpg  [1, 1, 1]   \n",
       "4   2741   2741.jpg  [1, 1, 1]   \n",
       "\n",
       "                                                text  \\\n",
       "0  I hate children \\n they are loud and messy and...   \n",
       "1  One simply does not run load bank cables \\n wi...   \n",
       "2  IF YOU DON'T FOLLOW THE \\n RULES \\n YOU'RE GON...   \n",
       "3         YEA LEMME GET THE \\n LAST SUPPER FADER \\n    \n",
       "4  Mrs. Reimus when someone gets \\n the right answer   \n",
       "\n",
       "                                             caption  \n",
       "0    a brown and white cat with blue eyes being held  \n",
       "1          a man with long hair and a leather jacket  \n",
       "2  if you dont follow the rules youre gonna have ...  \n",
       "3  a tattoo of the last supper and a black and wh...  \n",
       "4     a portrait of a man in a hat with his hand out  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_samples_frame = pd.read_json(test_path, lines=True)\n",
    "test_samples_frame.drop(test_samples_frame[test_samples_frame.text.str.split().str.len()>=20].index, axis = 0, inplace = True)\n",
    "test_samples_frame = test_samples_frame.reset_index(drop=True)\n",
    "test_samples_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Get the image features for a single image input\n",
    "def process_image_clip(in_img,datatype):\n",
    "    \n",
    "    image = Image.open(in_img).convert(\"RGB\")\n",
    "        \n",
    "    image_input = preprocess(image).to(device)\n",
    "    \n",
    "    return image_input\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Get the text features for a single text input\n",
    "def process_text_clip(in_text):\n",
    "    \n",
    "    text_input = clip.tokenize(in_text).squeeze().to(device)\n",
    "    return text_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import face_recognition\n",
    "def check(lab):\n",
    "    if lab == 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "with open(r\"autodl-tmp/Dataset/feature/num_face.json\", \"r\") as f:\n",
    "    num_face = eval(f.readlines()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MemesDatasetAug(torch.utils.data.Dataset):\n",
    "    \"\"\"Uses jsonl data to preprocess and serve\n",
    "    dictionary of multimodal tensors for model input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_path,\n",
    "            img_dir,\n",
    "            split_flag=None,\n",
    "    ):\n",
    "\n",
    "        self.samples_frame = pd.read_json(\n",
    "            data_path, lines=True\n",
    "        )\n",
    "        \n",
    "        self.samples_frame.drop(self.samples_frame[self.samples_frame.text.str.split().str.len()>=20].index, axis = 0, inplace = True)\n",
    "        \n",
    "        self.samples_frame = self.samples_frame.reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "        self.samples_frame.img = self.samples_frame.apply(\n",
    "            lambda row: (img_dir + '/' + row.img), axis=1\n",
    "        )\n",
    "          \n",
    "        self.datatype = split_flag\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"This method is called when you do len(instance)\n",
    "        for an instance of this class.\n",
    "        \"\"\"\n",
    "        return len(self.samples_frame)\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"This method is called when you do instance[key]\n",
    "        for an instance of this class.\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        img_id = self.samples_frame.loc[idx, \"id\"]\n",
    "        feature_index = int(img_id) - 1\n",
    "\n",
    "        img_file_name = self.samples_frame.loc[idx, \"img\"]\n",
    "\n",
    "        image_clip_input = process_image_clip(self.samples_frame.loc[idx, \"img\"],self.datatype)\n",
    "        # --------------------------------------------------------------------------------------\n",
    "        #         Pre-extracted features\n",
    "        if num_face[str(img_id)] == 0:\n",
    "            image_loc_feature = all_ROI[feature_index].to(device)\n",
    "        else:\n",
    "            image_loc_feature = torch.mean(torch.vstack([all_ROI[feature_index],all_face[feature_index]]), axis=0).to(device)\n",
    "            \n",
    "        text = self.samples_frame.loc[idx, \"text\"]\n",
    "        text_clip_input = process_text_clip(text)\n",
    "        text_drob_feature = all_ENT[feature_index].to(device)\n",
    "        cap_clip_input = process_text_clip(self.samples_frame.loc[idx, \"caption\"])\n",
    "        \n",
    "        if \"labels\" in self.samples_frame.columns:\n",
    "            #             Uncoment below for binary index creation\n",
    "            labels = self.samples_frame.loc[idx, \"labels\"]\n",
    "            label = torch.tensor(check(labels[0])).to(device)\n",
    "\n",
    "\n",
    "            sample = {\n",
    "                \"id\": img_id,\n",
    "                \"image_clip_input\": image_clip_input,\n",
    "                \"image_loc_feature\": image_loc_feature,\n",
    "                \"text_clip_input\": text_clip_input,\n",
    "                \"text_drob_embedding\": text_drob_feature,\n",
    "                \"cap_clip_input\": cap_clip_input,\n",
    "                \"label\": label,\n",
    "            }\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_train = MemesDatasetAug(train_path, data_dir, 'train')\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=128,\n",
    "                        shuffle=True, num_workers=0)\n",
    "dataset_val = MemesDatasetAug(dev_path, data_dir, 'val')\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=128,\n",
    "                        shuffle=False, num_workers=0)\n",
    "dataset_test = MemesDatasetAug(test_path, data_dir, 'test')\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=128,\n",
    "                        shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MVLP(nn.Module):\n",
    "    def __init__(self, n_out):\n",
    "        super(MVLP, self).__init__()\n",
    "\n",
    "        # 使用序列工具快速构建\n",
    "        self.linear1 = nn.Sequential(\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "        )\n",
    "\n",
    "        self.linear2 = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "        )\n",
    "\n",
    "        self.linear3 = nn.Sequential(\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "        )\n",
    "        \n",
    "        self.linear4 = nn.Sequential(\n",
    "            nn.AvgPool2d((32,1)),\n",
    "            nn.MaxPool2d((32,1)),\n",
    "            nn.Linear(3, 3),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(3, 3),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        \n",
    "        self.clip = copy.deepcopy(clip_model)\n",
    "        \n",
    "        self.gen_key_L1 = nn.Linear(512, 256) \n",
    "        self.gen_query_L1 = nn.Linear(512, 256)  \n",
    "        self.gen_key_L2 = nn.Linear(512, 256)  \n",
    "        self.gen_query_L2 = nn.Linear(512, 256)  \n",
    "        \n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "#         Initialize the global weights\n",
    "        self.w = nn.Parameter(torch.ones(3))  \n",
    "        self.fw = nn.Parameter(torch.ones(2)) \n",
    "\n",
    "        pre_output_layers = [nn.Linear(1024, 512), nn.ReLU()]\n",
    "        pre_output_layers.extend([nn.Linear(512, 128), nn.ReLU()])\n",
    "        self.fc_out = nn.Sequential(*pre_output_layers)\n",
    "        \n",
    "        self.out = nn.Linear(128, n_out)  \n",
    "\n",
    "    def selfatt_a(self, vec1, vec2):\n",
    "        q1 = F.relu(self.gen_query_L1(vec1))\n",
    "        k1 = F.relu(self.gen_key_L1(vec1))\n",
    "        q2 = F.relu(self.gen_query_L1(vec2))\n",
    "        k2 = F.relu(self.gen_key_L1(vec2))\n",
    "        score1 = torch.reshape(torch.bmm(q1.view(-1, 1, 256), k2.view(-1, 256, 1)), (-1, 1))\n",
    "        score2 = torch.reshape(torch.bmm(q2.view(-1, 1, 256), k1.view(-1, 256, 1)), (-1, 1))\n",
    "        wt_score1_score2_mat = torch.cat((score1, score2), 1)\n",
    "        wt_i1_i2 = self.soft(wt_score1_score2_mat.float())  # prob\n",
    "        prob_1 = wt_i1_i2[:, 0]\n",
    "        prob_2 = wt_i1_i2[:, 1]\n",
    "        wtd_i1 = vec1 * prob_1[:, None]\n",
    "        wtd_i2 = vec2 * prob_2[:, None]\n",
    "        out_rep = torch.cat((wtd_i1, wtd_i2), 1)\n",
    "        return out_rep \n",
    "\n",
    "    def selfatt_b(self, vec1, vec2):\n",
    "        q1 = F.relu(self.gen_query_L2(vec1))\n",
    "        k1 = F.relu(self.gen_key_L2(vec1))\n",
    "        q2 = F.relu(self.gen_query_L2(vec2))\n",
    "        k2 = F.relu(self.gen_key_L2(vec2))\n",
    "        score1 = torch.reshape(torch.bmm(q1.view(-1, 1, 256), k2.view(-1, 256, 1)), (-1, 1))\n",
    "        score2 = torch.reshape(torch.bmm(q2.view(-1, 1, 256), k1.view(-1, 256, 1)), (-1, 1))\n",
    "        wt_score1_score2_mat = torch.cat((score1, score2), 1)\n",
    "        wt_i1_i2 = self.soft(wt_score1_score2_mat.float())  # prob\n",
    "        prob_1 = wt_i1_i2[:, 0]\n",
    "        prob_2 = wt_i1_i2[:, 1]\n",
    "        wtd_i1 = vec1 * prob_1[:, None]\n",
    "        wtd_i2 = vec2 * prob_2[:, None]\n",
    "        out_rep = torch.cat((wtd_i1, wtd_i2), 1)\n",
    "        return out_rep\n",
    "    \n",
    "#     Adaptive Dynamic Fusion\n",
    "    def adf(self,img_feat,text_feat,cap_feat):\n",
    "        \n",
    "#         normalized global weights\n",
    "        w1 = torch.exp(self.w[0]) / torch.sum(torch.exp(self.w))\n",
    "        w2 = torch.exp(self.w[1]) / torch.sum(torch.exp(self.w))\n",
    "        w3 = torch.exp(self.w[2]) / torch.sum(torch.exp(self.w))\n",
    "        \n",
    "        fw1 = torch.exp(self.fw[0]) / torch.sum(torch.exp(self.fw))\n",
    "        fw2 = torch.exp(self.fw[1]) / torch.sum(torch.exp(self.fw))\n",
    "        \n",
    "#         local weight\n",
    "        out_wt = torch.cat((img_feat.unsqueeze(2),text_feat.unsqueeze(2),cap_feat.unsqueeze(2)),dim=2)\n",
    "        wt2 = self.linear4(out_wt.float()).reshape(out_wt.shape[0],3)\n",
    "        wt2 = F.normalize(torch.sigmoid(wt2.float()),p=1,dim=1)\n",
    "        att_wt = torch.chunk(wt2, 3, dim = 1)\n",
    "        \n",
    "#         final weight\n",
    "        out_img_txt1 = img_feat * att_wt[0] + text_feat * att_wt[1] + cap_feat * att_wt[2]\n",
    "        out_img_txt2 = img_feat * w1 + text_feat * w2 + cap_feat * w3\n",
    "    \n",
    "#         final fusion output\n",
    "#         out_img_txt = (out_img_txt1 + out_img_txt2)/2\n",
    "        out_img_txt = fw1*out_img_txt1 + fw2*out_img_txt2\n",
    "        \n",
    "        return out_img_txt\n",
    "    \n",
    "    def forward(self, in_CI, in_Loc, in_CT, in_Drob, in_Cap):\n",
    "        \n",
    "        in_CI = self.clip.encode_image(in_CI).data.float()\n",
    "        in_CT = self.clip.encode_text(in_CT).data.float()\n",
    "        in_Cap = self.clip.encode_text(in_Cap).data.float()\n",
    "        \n",
    "        Loc_feat = self.linear1(in_Loc)\n",
    "        Drob_feat = self.linear2(in_Drob)\n",
    "        Cap_feat = self.linear3(in_Cap)\n",
    "        \n",
    "        out_img = self.selfatt_a(Loc_feat, in_CI)\n",
    "        out_txt = self.selfatt_b(Drob_feat, in_CT)\n",
    "        \n",
    "        out_img_txt = self.adf(out_img,out_txt,Cap_feat)\n",
    "    \n",
    "        final_out = self.fc_out(out_img_txt)\n",
    "        out = torch.sigmoid(self.out(final_out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_size = 1 \n",
    "exp_name = \"best\"\n",
    "exp_path = \"checkpoint\" \n",
    "lr = 0.001\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "model = MVLP(output_size)\n",
    "model.to(device)\n",
    "\n",
    "# Frozen vision parameter\n",
    "for name, param in model.named_parameters():\n",
    "    if \"visual\" in name:\n",
    "        param.requires_grad = False\n",
    "    elif \"logit_scale\" in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Frozen language parameter\n",
    "# num = 0\n",
    "# for name, param in model.named_parameters():\n",
    "#     if \"clip.transformer\" in name:\n",
    "#         num += 1\n",
    "#         if num <= 14:\n",
    "#             param.requires_grad = False\n",
    "\n",
    "# Frozen all parameter\n",
    "# for name, param in model.named_parameters():\n",
    "#     if \"clip\" in name:\n",
    "#         param.requires_grad = False\n",
    "        \n",
    "# print(model)\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, patience, n_epochs):\n",
    "    epochs = n_epochs\n",
    "\n",
    "    train_acc_list=[]\n",
    "    val_acc_list=[]\n",
    "    train_loss_list=[]\n",
    "    val_loss_list=[]\n",
    "\n",
    "    # initialize the experiment path\n",
    "    Path(exp_path).mkdir(parents=True, exist_ok=True)\n",
    "    # initialize early_stopping object\n",
    "    chk_file = os.path.join(exp_path, 'checkpoint_'+exp_name+'.pt')\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True, path=chk_file)\n",
    "\n",
    "    model.train()\n",
    "    for i in range(epochs):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        total_train = 0\n",
    "        for data in dataloader_train:\n",
    "\n",
    "#             Clip features...\n",
    "            img_feat_clip = data['image_clip_input']\n",
    "            txt_feat_clip = data['text_clip_input']\n",
    "            cap_feat_clip = data['cap_clip_input']\n",
    "\n",
    "            img_feat_loc = data['image_loc_feature']\n",
    "            txt_feat_trans = data['text_drob_embedding']\n",
    "            \n",
    "            label = data['label'].to(device)\n",
    "            \n",
    "            \n",
    "            model.zero_grad(),\n",
    "            output = model(img_feat_clip, img_feat_loc, txt_feat_clip , txt_feat_trans , cap_feat_clip)\n",
    "            \n",
    "            loss = criterion(output.squeeze(), label.float())\n",
    "\n",
    "            loss.backward()\n",
    "           \n",
    "            optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = output.squeeze()\n",
    "                predicted_train = torch.where(output.data > 0.5, torch.ones_like(output.data), torch.zeros_like(output.data))\n",
    "                total_train += label.size(0)\n",
    "                total_acc_train += (predicted_train == label).sum().item()\n",
    "                total_loss_train += loss.item()\n",
    "\n",
    "        train_acc = 100 * total_acc_train/total_train\n",
    "        train_loss = total_loss_train/total_train\n",
    "        model.eval()\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "        total_val = 0\n",
    "        with torch.no_grad():\n",
    "            for data in dataloader_val:\n",
    "#                 Clip features...\n",
    "                img_feat_clip = data['image_clip_input']\n",
    "                txt_feat_clip = data['text_clip_input']\n",
    "                cap_feat_clip = data['cap_clip_input']\n",
    "\n",
    "                img_feat_loc = data['image_loc_feature']\n",
    "                txt_feat_trans = data['text_drob_embedding']\n",
    "\n",
    "                label = data['label'].to(device)\n",
    "\n",
    "                model.zero_grad()\n",
    "\n",
    "                output = model(img_feat_clip, img_feat_loc, txt_feat_clip , txt_feat_trans , cap_feat_clip)\n",
    "                \n",
    "                val_loss = criterion(output.squeeze(), label.float())\n",
    "                output = output.squeeze()\n",
    "                predicted_val = torch.where(output.data > 0.5, torch.ones_like(output.data), torch.zeros_like(output.data))\n",
    "                total_val += label.size(0)\n",
    "                total_acc_val += (predicted_val == label).sum().item()\n",
    "                total_loss_val += val_loss.item()\n",
    "        print(\"Saving model...\")\n",
    "\n",
    "        torch.save(model.state_dict(), os.path.join(exp_path, \"final.pt\"))\n",
    "\n",
    "        val_acc = 100 * total_acc_val/total_val\n",
    "        val_loss = total_loss_val/total_val\n",
    "\n",
    "        train_acc_list.append(train_acc)\n",
    "        val_acc_list.append(val_acc)\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "\n",
    "        early_stopping(val_loss, model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        print(f'Epoch {i+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
    "        # Dynamic output w weight transformation\n",
    "        for name, p in model.named_parameters():\n",
    "            if name == 'w':\n",
    "                print(\"特征权重: \", name)\n",
    "                w0 = (torch.exp(p[0]) / torch.sum(torch.exp(p))).item()\n",
    "                w1 = (torch.exp(p[1]) / torch.sum(torch.exp(p))).item()\n",
    "                w2 = (torch.exp(p[2]) / torch.sum(torch.exp(p))).item()\n",
    "#                 w3 = (torch.exp(p[3]) / torch.sum(torch.exp(p))).item()\n",
    "                print(\"w0={} w1={} w2={}\".format(w0, w1,w2))\n",
    "            if name == 'fw':\n",
    "                print(\"最终特征权重: \", name)\n",
    "                w0 = (torch.exp(p[0]) / torch.sum(torch.exp(p))).item()\n",
    "                w1 = (torch.exp(p[1]) / torch.sum(torch.exp(p))).item()\n",
    "                print(\"fw0={} fw1={}\".format(w0, w1))\n",
    "                print(\"\")\n",
    "        model.train()\n",
    "        scheduler.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # load the last checkpoint with the best model\n",
    "#     model.load_state_dict(torch.load(chk_file))\n",
    "\n",
    "    return  model, train_acc_list, val_acc_list, train_loss_list, val_loss_list, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "# early stopping patience; how long to wait after last time validation loss improved.\n",
    "patience = 20\n",
    "# model, train_acc_list, val_acc_list, train_loss_list, val_loss_list, epowsqec_num = train_model(model, patience, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_test = MVLP(output_size).to(device)\n",
    "# model_test.load_state_dict(torch.load('checkpoint/checkpoint_best.pt'))\n",
    "model_test.load_state_dict(torch.load('autodl-tmp/path_to_saved_files/checkpoint/final_8546.pt'))\n",
    "# model_test.load_state_dict(torch.load('checkpoint/final.pt'))\n",
    "def test_model(model):\n",
    "    model.eval()\n",
    "    total_acc_test = 0\n",
    "    total_loss_test = 0\n",
    "    total_test = 0\n",
    "    outputs = []\n",
    "    test_labels=[]\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader_test:\n",
    "            img_feat_clip = data['image_clip_input']\n",
    "            txt_feat_clip = data['text_clip_input']\n",
    "\n",
    "            img_feat_loc = data['image_loc_feature']\n",
    "            txt_feat_trans = data['text_drob_embedding']\n",
    "            cap_feat_clip = data['cap_clip_input']\n",
    "            label = data['label'].to(device)\n",
    "\n",
    "            out = model(img_feat_clip, img_feat_loc, txt_feat_clip, txt_feat_trans , cap_feat_clip)\n",
    "\n",
    "            outputs += list(out.cpu().data.numpy())\n",
    "            loss = criterion(out.squeeze(), label.float())\n",
    "            out = out.squeeze()\n",
    "            predicted_test = torch.where(out.data > 0.5, torch.ones_like(out.data), torch.zeros_like(out.data))\n",
    "            \n",
    "            total_test += label.size(0)\n",
    "            total_acc_test += (predicted_test == label).sum().item()\n",
    "            total_loss_test += loss.item()\n",
    "    print(total_acc_test)\n",
    "    \n",
    "    acc_test = total_acc_test/total_test\n",
    "    loss_test = total_loss_test/total_test\n",
    "    print(f'acc: {acc_test:.4f} loss: {loss_test:.4f}')\n",
    "    # Dynamic output w weight transformation\n",
    "    for name, p in model.named_parameters():\n",
    "        if name == 'w':\n",
    "            print(\"特征权重: \", name)\n",
    "            w0 = (torch.exp(p[0]) / torch.sum(torch.exp(p))).item()\n",
    "            w1 = (torch.exp(p[1]) / torch.sum(torch.exp(p))).item()\n",
    "            w2 = (torch.exp(p[2]) / torch.sum(torch.exp(p))).item()\n",
    "            print(\"w0={} w1={} w2={}\".format(w0, w1,w2))\n",
    "            print(\"\")\n",
    "        if name == 'fw':\n",
    "            print(\"最终特征权重: \", name)\n",
    "            w0 = (torch.exp(p[0]) / torch.sum(torch.exp(p))).item()\n",
    "            w1 = (torch.exp(p[1]) / torch.sum(torch.exp(p))).item()\n",
    "            print(\"fw0={} fw1={}\".format(w0, w1))\n",
    "            print(\"\")\n",
    "    return outputs\n",
    "outputs = test_model(model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_out = np.array(outputs)\n",
    "y_pred = np.zeros(np_out.shape)\n",
    "y_pred[np_out > 0.5] = 1\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "test_labels = []\n",
    "for index, row in test_samples_frame.iterrows():\n",
    "    lab = row['labels'][0]\n",
    "    if lab == 2:\n",
    "        test_labels.append(1)\n",
    "    else:\n",
    "        test_labels.append(0)        \n",
    "def calculate_mmae(expected, predicted, classes):\n",
    "    NUM_CLASSES = len(classes)\n",
    "    count_dict = {}\n",
    "    dist_dict = {}\n",
    "    for i in range(NUM_CLASSES):\n",
    "        count_dict[i] = 0\n",
    "        dist_dict[i] = 0.0\n",
    "    for i in range(len(expected)):\n",
    "        dist_dict[expected[i]] += abs(expected[i] - predicted[i])\n",
    "        count_dict[expected[i]] += 1\n",
    "    overall = 0.0\n",
    "    for claz in range(NUM_CLASSES):\n",
    "        if count_dict[claz] != 0:\n",
    "            class_dist = 1.0 * dist_dict[claz] / count_dict[claz]\n",
    "            overall += class_dist\n",
    "    overall /= NUM_CLASSES\n",
    "    #     return overall[0]\n",
    "    return overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rec = np.round(recall_score(test_labels, y_pred, average=\"macro\"), 4)\n",
    "prec = np.round(precision_score(test_labels, y_pred, average=\"macro\"), 4)\n",
    "f1 = np.round(f1_score(test_labels, y_pred, average=\"macro\"), 4)\n",
    "acc = np.round(accuracy_score(test_labels, y_pred), 4)\n",
    "mmae = np.round(calculate_mmae(test_labels, y_pred, [0, 1]), 4)\n",
    "mae = np.round(mean_absolute_error(test_labels, y_pred), 4)\n",
    "print(classification_report(test_labels, y_pred))\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "print(\"Acc, F1,  Rec, Prec, MAE, MMAE\")\n",
    "print(acc, f1, rec, prec, mae, mmae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
